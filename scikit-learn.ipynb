{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 documents - 3.98MB (training set)\n",
      "1353 documents - 2.87MB (test set)\n",
      "4 categories\n",
      "vectorize training done in 0.414s at 9.608MB/s\n",
      "n_samples: 2034, n_features: 7831\n",
      "vectorize testing done in 0.370s at 7.750MB/s\n",
      "n_samples: 1353, n_features: 7831\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode(\"utf-8\")) for s in docs) / 1e6\n",
    "\n",
    "\n",
    "def load_dataset(verbose=False, remove=()):\n",
    "    \"\"\"Load and vectorize the 20 newsgroups dataset.\"\"\"\n",
    "\n",
    "    data_train = fetch_20newsgroups(\n",
    "        subset=\"train\",\n",
    "        categories=categories,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "        remove=remove,\n",
    "    )\n",
    "\n",
    "    data_test = fetch_20newsgroups(\n",
    "        subset=\"test\",\n",
    "        categories=categories,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "        remove=remove,\n",
    "    )\n",
    "\n",
    "    # order of labels in `target_names` can be different from `categories`\n",
    "    target_names = data_train.target_names\n",
    "\n",
    "    # split target in a training set and a test set\n",
    "    y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "    # Extracting features from the training data using a sparse vectorizer\n",
    "    t0 = time()\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\"\n",
    "    )\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "    duration_train = time() - t0\n",
    "\n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    t0 = time()\n",
    "    X_test = vectorizer.transform(data_test.data)\n",
    "    duration_test = time() - t0\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    if verbose:\n",
    "        # compute size of loaded data\n",
    "        data_train_size_mb = size_mb(data_train.data)\n",
    "        data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "        print(\n",
    "            f\"{len(data_train.data)} documents - \"\n",
    "            f\"{data_train_size_mb:.2f}MB (training set)\"\n",
    "        )\n",
    "        print(f\"{len(data_test.data)} documents - {data_test_size_mb:.2f}MB (test set)\")\n",
    "        print(f\"{len(target_names)} categories\")\n",
    "        print(\n",
    "            f\"vectorize training done in {duration_train:.3f}s \"\n",
    "            f\"at {data_train_size_mb / duration_train:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_train.shape[0]}, n_features: {X_train.shape[1]}\")\n",
    "        print(\n",
    "            f\"vectorize testing done in {duration_test:.3f}s \"\n",
    "            f\"at {data_test_size_mb / duration_test:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_test.shape[0]}, n_features: {X_test.shape[1]}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, feature_names, target_names\n",
    "\n",
    "X_train, X_test, y_train, y_test, feature_names, target_names = load_dataset(\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 49 stored elements and shape (1, 7831)>\n",
      "  Coords\tValues\n",
      "  (0, 2517)\t0.18882559857168102\n",
      "  (0, 453)\t0.14810906626622972\n",
      "  (0, 7330)\t0.13783664104130097\n",
      "  (0, 254)\t0.34404631756778353\n",
      "  (0, 2276)\t0.07033982375105988\n",
      "  (0, 7070)\t0.2944436581666543\n",
      "  (0, 6173)\t0.17819540353245403\n",
      "  (0, 166)\t0.08658058224839943\n",
      "  (0, 3457)\t0.09233043938439052\n",
      "  (0, 7504)\t0.06709283290470455\n",
      "  (0, 4906)\t0.13276131063021313\n",
      "  (0, 6245)\t0.17819540353245403\n",
      "  (0, 4650)\t0.10645464205726671\n",
      "  (0, 4406)\t0.1270798399040038\n",
      "  (0, 5370)\t0.13150813563613234\n",
      "  (0, 1353)\t0.12609338927283967\n",
      "  (0, 2927)\t0.24787945867395397\n",
      "  (0, 3223)\t0.08031114792141571\n",
      "  (0, 2133)\t0.1369871763301335\n",
      "  (0, 5453)\t0.09571658744266878\n",
      "  (0, 5091)\t0.23464700210776418\n",
      "  (0, 5454)\t0.12336340331196224\n",
      "  (0, 2395)\t0.09339764418615404\n",
      "  (0, 4048)\t0.05087600635720216\n",
      "  (0, 3720)\t0.1320941585100957\n",
      "  (0, 6777)\t0.13858629940851536\n",
      "  (0, 2777)\t0.12336340331196224\n",
      "  (0, 6209)\t0.06755182000930571\n",
      "  (0, 4398)\t0.13276131063021313\n",
      "  (0, 6248)\t0.13031925497276003\n",
      "  (0, 4215)\t0.05028166375780262\n",
      "  (0, 437)\t0.08408590888081635\n",
      "  (0, 5818)\t0.07113761884486677\n",
      "  (0, 6171)\t0.10954964487469154\n",
      "  (0, 3039)\t0.17262393395199271\n",
      "  (0, 943)\t0.08169120378397438\n",
      "  (0, 2568)\t0.08588864754120627\n",
      "  (0, 7556)\t0.12092134765450917\n",
      "  (0, 2634)\t0.10645464205726671\n",
      "  (0, 4078)\t0.09877441608795796\n",
      "  (0, 2199)\t0.09396173192159102\n",
      "  (0, 5671)\t0.13858629940851536\n",
      "  (0, 7019)\t0.11669548195458879\n",
      "  (0, 272)\t0.1954608165475199\n",
      "  (0, 221)\t0.16930557057795925\n",
      "  (0, 332)\t0.14882713930583744\n",
      "  (0, 233)\t0.15822504662408832\n",
      "  (0, 7377)\t0.10908138467880522\n",
      "  (0, 2883)\t0.09372154229695666\n",
      "1\n",
      "00\n",
      "alt.atheism\n",
      "(2034, 7831)\n",
      "(1353, 7831)\n",
      "(2034,)\n",
      "(1353,)\n"
     ]
    }
   ],
   "source": [
    "# print some data\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "print(feature_names[0])\n",
    "print(target_names[0])\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.utils.extmath import density\n",
    "\n",
    "\n",
    "def benchmark(clf, custom_name=False):\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(f\"train time: {train_time:.3}s\")\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(f\"test time:  {test_time:.3}s\")\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(f\"accuracy:   {score:.3}\")\n",
    "\n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        print(f\"dimensionality: {clf.coef_.shape[1]}\")\n",
    "        print(f\"density: {density(clf.coef_)}\")\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    if custom_name:\n",
    "        clf_descr = str(custom_name)\n",
    "    else:\n",
    "        clf_descr = clf.__class__.__name__\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LogisticRegression(C=5, max_iter=1000)\n",
      "train time: 1.08s\n",
      "test time:  0.001s\n",
      "accuracy:   0.892\n",
      "dimensionality: 7831\n",
      "density: 1.0\n",
      "\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(n_neighbors=100)\n",
      "train time: 0.000999s\n",
      "test time:  0.111s\n",
      "accuracy:   0.864\n",
      "\n",
      "================================================================================\n",
      "Complement naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "ComplementNB(alpha=0.1)\n",
      "train time: 0.002s\n",
      "test time:  0.00101s\n",
      "accuracy:   0.898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "    (LogisticRegression(C=5, max_iter=1000), \"Logistic Regression\"),\n",
    "    (KNeighborsClassifier(n_neighbors=100), \"kNN\"),\n",
    "    (ComplementNB(alpha=0.1), \"Complement naive Bayes\"),\n",
    "\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups training data\n",
      "3803 documents - 6.245MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"sci.space\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups training data\")\n",
    "raw_data, _ = fetch_20newsgroups(subset=\"train\", categories=categories, return_X_y=True)\n",
    "data_size_mb = sum(len(s.encode(\"utf-8\")) for s in raw_data) / 1e6\n",
    "print(f\"{len(raw_data)} documents - {data_size_mb:.3f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(doc):\n",
    "    \"\"\"Extract tokens from doc.\n",
    "\n",
    "    This uses a simple regex that matches word characters to break strings\n",
    "    into tokens. For a more principled approach, see CountVectorizer or\n",
    "    TfidfVectorizer.\n",
    "    \"\"\"\n",
    "    return (tok.lower() for tok in re.findall(r\"\\w+\", doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def token_freqs(doc):\n",
    "    \"\"\"Extract a dict mapping tokens from doc to their occurrences.\"\"\"\n",
    "\n",
    "    freq = defaultdict(int)\n",
    "    for tok in tokenize(doc):\n",
    "        freq[tok] += 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.173 s at 5.3 MB/s\n",
      "Found 47928 unique terms\n",
      "defaultdict(<class 'int'>, {'subject': 1, 're': 1, 'christian': 1, 'daemons': 1, 'biblical': 1, 'demons': 1, 'the': 17, 'u': 1, 'from': 3, 'stigaard': 4, 'mhd': 2, 'moorhead': 6, 'msus': 3, 'edu': 3, 'reply': 1, 'to': 4, 'organization': 1, 'state': 2, 'university': 2, 'mn': 1, 'nntp': 1, 'posting': 1, 'host': 1, '134': 1, '29': 1, '97': 1, '2': 1, 'lines': 1, '23': 1, '667': 4, 'neighbor': 4, 'of': 5, 'beast': 7, 'no': 2, 'is': 7, 'across': 3, 'street': 2, '664': 1, 'and': 3, '668': 2, 'are': 2, 'neighbors': 1, 'i': 2, 'think': 2, 'some': 1, 'people': 1, 'still': 2, 'not': 2, 'clear': 1, 'on': 1, 'this': 3, 'but': 1, 'rather': 1, 'it': 2, 'in': 1, 'fact': 1, 'which': 1, 'sheesh': 1, 'didn': 1, 't': 2, 'you': 2, 'know': 1, '666': 1, 's': 1, 'apartment': 1, 'hall': 1, 'his': 1, 'along': 1, 'with': 2, 'rest': 1, '6th': 1, 'floor': 1, 'justin': 1, 'trying': 1, 'figure': 1, 'out': 1, 'what': 1, 'has': 1, 'do': 1, 'alt': 1, 'discordia': 1, 'doesn': 1, 'seem': 1, 'discordant': 1, 'paul': 1, 'w': 1, 'lokean': 1, 'discordian': 1, 'libertarian': 1, 'xoa': 1, 'internet': 1, 'mhd1': 1, 'fnord': 1, 'episkopos': 1, 'chair': 1, 'campus': 1, 'discordians': 1, 'rectal': 1, 'neufotomist': 1, 'at': 1, 'large': 1, 'if': 1, 'left': 1, 'a': 1, 'quote': 1, 'here': 1, 'someone': 1, 'would': 1, 'meant': 1, 'something': 1})\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 110 stored elements and shape (1, 47928)>\n",
      "  Coords\tValues\n",
      "  (0, 1087)\t1.0\n",
      "  (0, 2477)\t1.0\n",
      "  (0, 2875)\t1.0\n",
      "  (0, 3295)\t1.0\n",
      "  (0, 5011)\t1.0\n",
      "  (0, 5018)\t1.0\n",
      "  (0, 5021)\t4.0\n",
      "  (0, 5022)\t2.0\n",
      "  (0, 5151)\t1.0\n",
      "  (0, 6310)\t1.0\n",
      "  (0, 6783)\t1.0\n",
      "  (0, 7232)\t3.0\n",
      "  (0, 8006)\t1.0\n",
      "  (0, 8031)\t1.0\n",
      "  (0, 8267)\t3.0\n",
      "  (0, 8513)\t1.0\n",
      "  (0, 8788)\t2.0\n",
      "  (0, 9175)\t1.0\n",
      "  (0, 10024)\t7.0\n",
      "  (0, 10362)\t1.0\n",
      "  (0, 11562)\t1.0\n",
      "  (0, 12067)\t1.0\n",
      "  (0, 12692)\t1.0\n",
      "  (0, 13015)\t1.0\n",
      "  (0, 13266)\t1.0\n",
      "  :\t:\n",
      "  (0, 38970)\t1.0\n",
      "  (0, 39389)\t1.0\n",
      "  (0, 40335)\t1.0\n",
      "  (0, 40343)\t1.0\n",
      "  (0, 40351)\t1.0\n",
      "  (0, 41141)\t2.0\n",
      "  (0, 41305)\t4.0\n",
      "  (0, 41309)\t2.0\n",
      "  (0, 41469)\t2.0\n",
      "  (0, 41623)\t1.0\n",
      "  (0, 42343)\t2.0\n",
      "  (0, 42976)\t17.0\n",
      "  (0, 43105)\t2.0\n",
      "  (0, 43127)\t3.0\n",
      "  (0, 43461)\t4.0\n",
      "  (0, 44112)\t1.0\n",
      "  (0, 44368)\t1.0\n",
      "  (0, 44885)\t2.0\n",
      "  (0, 46152)\t1.0\n",
      "  (0, 46654)\t1.0\n",
      "  (0, 46695)\t1.0\n",
      "  (0, 46965)\t2.0\n",
      "  (0, 47163)\t1.0\n",
      "  (0, 47438)\t1.0\n",
      "  (0, 47669)\t2.0\n",
      "(1, 47928)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dict_count_vectorizers = defaultdict(list)\n",
    "\n",
    "t0 = time()\n",
    "vectorizer = DictVectorizer()\n",
    "vectorizer.fit_transform(token_freqs(d) for d in raw_data)\n",
    "duration = time() - t0\n",
    "dict_count_vectorizers[\"vectorizer\"].append(\n",
    "    vectorizer.__class__.__name__ + \"\\non freq dicts\"\n",
    ")\n",
    "dict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\n",
    "print(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\n",
    "print(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")\n",
    "\n",
    "# print some data\n",
    "print(token_freqs(raw_data[0]))\n",
    "print(vectorizer.transform(token_freqs(raw_data[0])))\n",
    "print(vectorizer.transform(token_freqs(raw_data[0])).toarray().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "\n",
      "['and' 'and this' 'and this is' 'document' 'document is' 'document is the'\n",
      " 'first' 'first document' 'is' 'is the' 'is the first' 'is the second'\n",
      " 'is the third' 'is this' 'is this the' 'one' 'second' 'second document'\n",
      " 'the' 'the first' 'the first document' 'the second' 'the second document'\n",
      " 'the third' 'the third one' 'third' 'third one' 'this' 'this document'\n",
      " 'this document is' 'this is' 'this is the' 'this the' 'this the first']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print()\n",
    "print(X.toarray())\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print()\n",
    "print(vectorizer2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bar' 'baz' 'foo']\n",
      "[[2. 0. 1.]\n",
      " [0. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = v.fit_transform(D)\n",
    "print(v.get_feature_names_out())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1]), array([0, 1, 2]), array([0, 1, 2, 3])]\n",
      "[[1. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding with example text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoded Matrix:\n",
      "[[0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "Document One-Hot Vector Representation:\n",
      "[1. 2. 1. 1. 1. 1.]\n",
      "\n",
      "Feature Names:\n",
      "['x0_cool' 'x0_document' 'x0_is' 'x0_second' 'x0_the' 'x0_this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Define the vocabulary and document\n",
    "vocabulary = [\"document\", \"this\", \"here\", \"one\", \"is\", \"yet\", \"another\", \"third\", \"second\", \"and\", \"the\", \"first\"]\n",
    "document = [\"this\", \"document\", \"is\", \"the\", \"second\", \"cool\", \"document\"]\n",
    "\n",
    "# Create a OneHotEncoder with a fixed vocabulary\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Transform the document into a 2D array for encoding\n",
    "# Each word should be treated as a separate \"feature\" for the encoder\n",
    "document_array = np.array(document).reshape(-1, 1)\n",
    "\n",
    "# Apply One-Hot-Encoding\n",
    "encoded_document = encoder.fit_transform(document_array)\n",
    "print(\"One-Hot Encoded Matrix:\")\n",
    "print(encoded_document)\n",
    "\n",
    "# Sum over rows to create a single vector representation for the document\n",
    "document_vector = encoded_document.sum(axis=0)\n",
    "print(\"\\nDocument One-Hot Vector Representation:\")\n",
    "print(document_vector)\n",
    "\n",
    "# Show feature names corresponding to the vector\n",
    "print(\"\\nFeature Names:\")\n",
    "print(encoder.get_feature_names_out())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
